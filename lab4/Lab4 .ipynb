{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Extending Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Mangling:\n",
    "\n",
    "Mangling of the CSV:\n",
    "- We first removed the imdb link from the csv because we knew we would never need to use that (**Note: this was the only feature removed from the csv**)\n",
    "- We then went through and deleted all of the movies that were made in another country (foriegn films) we did this because we wanted to just look at American films, also because the currency units for those countries (for budget and gross) were in native currency units, not USD, and with changing exchange rates, it's not very easy to compare across countries.\n",
    "- We then went through and converted all 0 values for gross, movie_facebook_likes, and director_facebook_likes to a blank value in the csv (so that it is read in as NaN by pandas), this is so that we cna more easily impute values later. Note: according to the description on the kaggle entry, because of the way the data was scraped, some movies had missing data. The Python scraper just made these values into a 0 instead of NaN.\n",
    "- We then removed all movies with an undefined gross. Being the feature we are trying to predict, we should not be imputing values for gross to train our model. That will basically reduce our model to an imputation algorithm...\n",
    "- We then removed all movies that were made before 1935. We did this because there were only a handful of movies ranging from 1915 to 1935, the way we are classifying budget (described below) would not work with a small sample of movies from that time period. We could have cut this number at a different year (say 1960), but we didn't want to exclude such classics as \"Bambi\" or \"Gone With the Wind\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mangling of the Data:\n",
    "- After the above steps, we made more edits to the data using pandas. First, we removed features that we thought would be un-useful to our prediction algorithm. We removed all features concerning facebook likes. We did this because a significant portion of the movies in the training set debuted before facebook was invented and widely adopted. While some of these movies have received retroactive \"likes\" on facebook, only the most famous classics received a substantial amount of retraoctive \"likes\". Most lesser known films received very low amounts of \"likes\" (presumably because modern movie watchers don't really care to search for lesser known movies on facebook, or because the movie doesn't have a facebook). For this reason we decided to remove movie_facebook_likes\n",
    "- Likewise, we removed the other \"likes\" for the same reasons as above. For example, the esteemed director George Lucas has a total of 0 \"likes\" between all of his films. This feature obviously would not help us predict the profitability of movies.\n",
    "- We also removed irrelevant information such as aspect_ratio, language, and country. Because we deleted all foreign films the country will always be USA. A simple filter of the data reveals that there are no more than 20 movies made in the US that use a language other than English, therefore there is not enough data to use language as training feature. However, we did not delete the movies in a different language, because most of them were famous films such as *Letters from Iwo Jima* and *The Kite Runner*. We still count them as a valuable part of the dataset, just don't find the language of particular value. Lastly, we removed aspect_ratio because that seems to be unimportant for predicting the success of a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3232 entries, 0 to 3231\n",
      "Data columns (total 18 columns):\n",
      "color                     3231 non-null object\n",
      "director_name             3232 non-null object\n",
      "num_critic_for_reviews    3229 non-null float64\n",
      "duration                  3231 non-null float64\n",
      "actor_2_name              3229 non-null object\n",
      "gross                     3232 non-null int64\n",
      "genres                    3232 non-null object\n",
      "actor_1_name              3230 non-null object\n",
      "movie_title               3232 non-null object\n",
      "num_voted_users           3232 non-null int64\n",
      "actor_3_name              3225 non-null object\n",
      "facenumber_in_poster      3226 non-null float64\n",
      "plot_keywords             3208 non-null object\n",
      "num_user_for_reviews      3231 non-null float64\n",
      "content_rating            3206 non-null object\n",
      "budget                    3071 non-null float64\n",
      "title_year                3232 non-null int64\n",
      "imdb_score                3232 non-null float64\n",
      "dtypes: float64(6), int64(3), object(9)\n",
      "memory usage: 454.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"movie_metadata.csv\")\n",
    "for x in ['movie_facebook_likes', 'director_facebook_likes', 'actor_2_facebook_likes', \n",
    "          'actor_1_facebook_likes','actor_3_facebook_likes', 'cast_total_facebook_likes',\n",
    "          'aspect_ratio', 'language', 'country']:\n",
    "    if x in df:\n",
    "        del df[x]\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tamper with the groupings to improve imputations? How do we improve how many values get imputed?\n",
    "df_grouped = df.groupby(by=['director_name'])\n",
    "# director_name adds about 50 rows (imputes about 50 rows and then deletes about 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3232 entries, 0 to 3231\n",
      "Data columns (total 18 columns):\n",
      "num_critic_for_reviews    3229 non-null float64\n",
      "duration                  3231 non-null float64\n",
      "gross                     3232 non-null int64\n",
      "num_voted_users           3232 non-null int64\n",
      "facenumber_in_poster      3232 non-null float64\n",
      "num_user_for_reviews      3231 non-null float64\n",
      "budget                    3159 non-null float64\n",
      "title_year                3232 non-null int64\n",
      "imdb_score                3232 non-null float64\n",
      "plot_keywords             3208 non-null object\n",
      "actor_2_name              3229 non-null object\n",
      "color                     3231 non-null object\n",
      "actor_1_name              3230 non-null object\n",
      "genres                    3232 non-null object\n",
      "actor_3_name              3225 non-null object\n",
      "movie_title               3232 non-null object\n",
      "director_name             3232 non-null object\n",
      "content_rating            3206 non-null object\n",
      "dtypes: float64(6), int64(3), object(9)\n",
      "memory usage: 454.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_imputed = df_grouped.transform(lambda grp: grp.fillna(grp.median()))\n",
    "col_deleted = list( set(df.columns) - set(df_imputed.columns)) #in case the median op deleted columns\n",
    "df_imputed[col_deleted] = df[col_deleted]\n",
    "\n",
    "print(df_imputed.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3119 entries, 0 to 3230\n",
      "Data columns (total 18 columns):\n",
      "num_critic_for_reviews    3119 non-null float64\n",
      "duration                  3119 non-null float64\n",
      "gross                     3119 non-null int64\n",
      "num_voted_users           3119 non-null int64\n",
      "facenumber_in_poster      3119 non-null float64\n",
      "num_user_for_reviews      3119 non-null float64\n",
      "budget                    3119 non-null float64\n",
      "title_year                3119 non-null int64\n",
      "imdb_score                3119 non-null float64\n",
      "plot_keywords             3119 non-null object\n",
      "actor_2_name              3119 non-null object\n",
      "color                     3119 non-null object\n",
      "actor_1_name              3119 non-null object\n",
      "genres                    3119 non-null object\n",
      "actor_3_name              3119 non-null object\n",
      "movie_title               3119 non-null object\n",
      "director_name             3119 non-null object\n",
      "content_rating            3119 non-null object\n",
      "dtypes: float64(6), int64(3), object(9)\n",
      "memory usage: 463.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# drop rows that still have missing values after imputation\n",
    "df_imputed.dropna(inplace=True)\n",
    "print (df_imputed.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Up the Data Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2495 624\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "train_set, test_set = model_selection.train_test_split(df_imputed, test_size=0.2, train_size=0.8, random_state= 101)\n",
    "print(len(train_set), len(test_set))\n",
    "## http://localhost:8888/notebooks/fork/MachineLearningNotebooks/05.%20Logistic%20Regression.ipynb#Training-and-Testing-Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT IS CROSS VALIDATION???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer...It's performed automatically above^^ with the train_test_split, we just don't have 3 iterations to use later. Which is fine for now, Larson just said we will be doing it in later labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# But First... A Super Cool Custom Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirement**: [20 points] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template used in the course. You should add the following functionality to the logistic regression classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StochasticBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import pinv\n",
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001, optimizer=\"steep_desc\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.optimizer = optimizer\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            if self.optimizer == \"steep_desc\":\n",
    "                blr = BinaryLogisticRegression(self.eta, self.iters, self.C) #??\n",
    "            elif self.optimizer == \"stoch_grad\":\n",
    "                blr = StochasticBinaryLogisticRegression(self.eta, self.iters, self.C)\n",
    "            elif self.optimizer == \"newton\":\n",
    "                blr = HessianBinaryLogisticRegression(self.eta, self.iters, self.C)\n",
    "                \n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,bin_class.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T    \n",
    "       \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier  ##go through the list of classifiers and get predictions for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "**Requirement**: Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[  0.0568186 ]\n",
      " [ 11.09977003]\n",
      " [ -2.83792666]\n",
      " [ 28.01308577]\n",
      " [ 12.18540831]]\n",
      "Accuracy of:  0.333333333333\n",
      "--------------------\n",
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.5192749   0.54260366  1.83423215 -2.85103755 -1.27673909]\n",
      " [ 2.74863796  1.40523315 -3.65010918  0.74759963 -3.8986662 ]\n",
      " [-2.85724901 -3.13527515 -3.15521412  4.98366131  4.87121737]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 2 2 1 1 2 1 2 1 2 1 1 2 1 1 1 2 1 2 1\n",
      " 1 1 1 2 2 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Accuracy of:  0.913333333333\n",
      "----------------\n",
      "<class 'numpy.ndarray'>\n",
      "(150,)\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Linear Regression Testing\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = ds.target # note problem is NOT binary anymore, there are three classes!\n",
    "# y = (ds.target>1).astype(np.int) # make problem binary\n",
    "\n",
    "blr = BinaryLogisticRegression(0.1, 200, C=.001)\n",
    "blr.fit(X,y)\n",
    "print(blr)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat)) \n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "lr = LogisticRegression(0.1,3000,C=0.001, optimizer=\"stoch_grad\")\n",
    "lr.fit(X,y)\n",
    "print(lr)\n",
    "yhat = lr.predict(X)\n",
    "\n",
    "print(yhat)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))\n",
    "\n",
    "print('----------------')\n",
    "print(type(X))\n",
    "print(y.shape)\n",
    "# print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.30000000e+01   1.02000000e+02   1.99860000e+04 ...,   4.00000000e+07\n",
      "    2.00100000e+03   6.20000000e+00]\n",
      " [  1.50000000e+01   8.70000000e+01   2.30800000e+03 ...,   4.00000000e+06\n",
      "    1.98700000e+03   4.80000000e+00]\n",
      " [  8.50000000e+01   1.06000000e+02   2.26490000e+04 ...,   2.10000000e+07\n",
      "    2.00100000e+03   6.30000000e+00]\n",
      " ..., \n",
      " [  1.49000000e+02   1.35000000e+02   7.70290000e+04 ...,   6.00000000e+07\n",
      "    2.00300000e+03   5.40000000e+00]\n",
      " [  5.00000000e+01   9.10000000e+01   1.21880000e+04 ...,   3.50000000e+06\n",
      "    2.00400000e+03   5.30000000e+00]\n",
      " [  1.60000000e+01   1.02000000e+02   2.29500000e+03 ...,   2.10000000e+07\n",
      "    1.99900000e+03   5.80000000e+00]]\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(0.1, 1000, C=0.001, optimizer=\"stoch_grad\")\n",
    "# print(train_set)\n",
    "X = train_set.drop(['gross', 'movie_title', 'director_name', 'plot_keywords', 'actor_1_name', 'actor_3_name', 'genres',\n",
    "                   'actor_2_name', 'content_rating', 'color'], axis=1).values\n",
    "y = train_set['gross'].values\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1978 1978 1978 ..., 1978 1978 1978]\n",
      "Accuracy of:  0.0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr.fit(X,y)\n",
    "yhat = lr.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6114237   800000 40219708 ..., 31111260    96793    15593]\n",
      "[1978 1978 1978 ..., 1978 1978 1978]\n",
      "Accuracy of:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(yhat)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
