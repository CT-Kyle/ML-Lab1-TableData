{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (40 points total)\n",
    "### [20 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "raw_df = pd.read_csv('./south-park-dialogue/All-seasons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing all characters with less than 100 lines\n",
      "Total Characters to Remove: 3885\n",
      "Total Characters Left:  65\n",
      "Leftover Classes:\n",
      "['Announcer' 'Barbrady' 'Bebe' 'Butters' 'Cartman' 'Chef' 'Chris' 'Clerk'\n",
      " 'Clyde' 'Coon' 'Craig' 'Crowd' 'Doctor' 'Dr. Doctor' 'General' 'Gerald'\n",
      " 'Ike' 'Jesus' 'Jimbo' 'Jimmy' 'Kenny' 'Kids' 'Kyle' 'Liane' 'Linda' 'Man'\n",
      " 'Man 1' 'Man 2' 'Mark' 'Mayor' 'Mayor McDaniels' 'Mephesto' 'Michael'\n",
      " 'Mr. Garrison' 'Mr. Hankey' 'Mr. Mackey' 'Mrs. Garrison' 'Ms. Choksondik'\n",
      " 'Mysterion' 'Narrator' 'Ned' 'Officer Barbrady' 'Pete' 'Phillip' 'Pip'\n",
      " 'Principal Victoria' 'Randy' 'Reporter' 'Satan' 'Scott' 'Sharon' 'Sheila'\n",
      " 'Shelly' 'Stan' 'Stephen' 'Stuart' 'Terrance' 'The Boys' 'Timmy' 'Token'\n",
      " 'Towelie' 'Tweek' 'Wendy' 'Woman' 'Yates']\n"
     ]
    }
   ],
   "source": [
    "all_classes = raw_df['Character'].values\n",
    "df = pd.DataFrame(raw_df.drop(['Character', 'Season', 'Episode'], axis=1).values, index=raw_df['Character'], columns=['Line'])\n",
    "\n",
    "uniques, counts = np.unique(all_classes, return_counts=True)\n",
    "num_classes = len(uniques)\n",
    "\n",
    "to_delete = []\n",
    "names_to_delete = []\n",
    "\n",
    "character_line_minimum = 100\n",
    "\n",
    "for i, idx in enumerate(counts):\n",
    "    if idx < character_line_minimum:\n",
    "        names_to_delete.append(uniques[i])\n",
    "    \n",
    "print(\"Removing all characters with less than\", character_line_minimum, \"lines\")\n",
    "print(\"Total Characters to Remove:\",len(names_to_delete))\n",
    "print(\"Total Characters Left: \",len(uniques) - len(names_to_delete))\n",
    "\n",
    "df = df.drop(names_to_delete, axis=0)\n",
    "\n",
    "new_uniques, y_ints, counts = np.unique(df.index.values, return_inverse=True,return_counts=True)\n",
    "num_classes = len(new_uniques)\n",
    "print(\"Leftover Classes:\")\n",
    "print(new_uniques)\n",
    "y_ohe = keras.utils.to_categorical(y_ints, num_classes)\n",
    "X_preprep = df['Line'].values\n",
    "y_values = df.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None\n",
    "padding = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X_preprep)\n",
    "sequences = tokenizer.texts_to_sequences(X_preprep)\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "embed_size = 100\n",
    "\n",
    "f = open('glove.6B/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2, stratify=y_values, random_state=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (50 points total)\n",
    "### [25 points] Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Adjust hyper-parameters of the networks as needed to improve generalization performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embed_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=padding,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1966100   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 65)                8385      \n",
      "=================================================================\n",
      "Total params: 2,091,733\n",
      "Trainable params: 125,633\n",
      "Non-trainable params: 1,966,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Wall time: 875 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "lstm_rnn = Sequential()\n",
    "lstm_rnn.add(embedding_layer)\n",
    "lstm_rnn.add(LSTM(128, dropout=.2, recurrent_dropout=.2))\n",
    "lstm_rnn.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "lstm_rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(lstm_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37396 samples, validate on 9349 samples\n",
      "Epoch 1/10\n",
      "37396/37396 [==============================] - 115s - loss: 3.0130 - acc: 0.2085 - val_loss: 2.9559 - val_acc: 0.2091\n",
      "Epoch 2/10\n",
      "37396/37396 [==============================] - 125s - loss: 2.9032 - acc: 0.2189 - val_loss: 2.8520 - val_acc: 0.2265\n",
      "Epoch 3/10\n",
      "37396/37396 [==============================] - 142s - loss: 2.8238 - acc: 0.2342 - val_loss: 2.7870 - val_acc: 0.2459\n",
      "Epoch 4/10\n",
      "37396/37396 [==============================] - 149s - loss: 2.7599 - acc: 0.2501 - val_loss: 2.7578 - val_acc: 0.2624\n",
      "Epoch 5/10\n",
      "37396/37396 [==============================] - 149s - loss: 2.7062 - acc: 0.2629 - val_loss: 2.6956 - val_acc: 0.2724\n",
      "Epoch 6/10\n",
      "37396/37396 [==============================] - 140s - loss: 2.6513 - acc: 0.2740 - val_loss: 2.6660 - val_acc: 0.2777\n",
      "Epoch 7/10\n",
      "37396/37396 [==============================] - 139s - loss: 2.6090 - acc: 0.2864 - val_loss: 2.6384 - val_acc: 0.2854\n",
      "Epoch 8/10\n",
      "37396/37396 [==============================] - 140s - loss: 2.5728 - acc: 0.2940 - val_loss: 2.6148 - val_acc: 0.2882\n",
      "Epoch 9/10\n",
      "37396/37396 [==============================] - 135s - loss: 2.5364 - acc: 0.3035 - val_loss: 2.6280 - val_acc: 0.2800\n",
      "Epoch 10/10\n",
      "37396/37396 [==============================] - 124s - loss: 2.5099 - acc: 0.3065 - val_loss: 2.5795 - val_acc: 0.3036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21627271668>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1966100   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               87936     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 65)                8385      \n",
      "=================================================================\n",
      "Total params: 2,062,421\n",
      "Trainable params: 96,321\n",
      "Non-trainable params: 1,966,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "gru_rnn = Sequential()\n",
    "gru_rnn.add(embedding_layer)\n",
    "gru_rnn.add(GRU(128, dropout=.2, recurrent_dropout=.2))\n",
    "gru_rnn.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "gru_rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(gru_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37396 samples, validate on 9349 samples\n",
      "Epoch 1/10\n",
      "37396/37396 [==============================] - 101s - loss: 3.0059 - acc: 0.2082 - val_loss: 2.9191 - val_acc: 0.2091\n",
      "Epoch 2/10\n",
      "37396/37396 [==============================] - 100s - loss: 2.8661 - acc: 0.2165 - val_loss: 2.8261 - val_acc: 0.2263\n",
      "Epoch 3/10\n",
      "37396/37396 [==============================] - 100s - loss: 2.7721 - acc: 0.2423 - val_loss: 2.7404 - val_acc: 0.2554\n",
      "Epoch 4/10\n",
      "37396/37396 [==============================] - 105s - loss: 2.6931 - acc: 0.2675 - val_loss: 2.7075 - val_acc: 0.2708\n",
      "Epoch 5/10\n",
      "37396/37396 [==============================] - 100s - loss: 2.6309 - acc: 0.2828 - val_loss: 2.6369 - val_acc: 0.2844\n",
      "Epoch 6/10\n",
      "37396/37396 [==============================] - 100s - loss: 2.5856 - acc: 0.2902 - val_loss: 2.6055 - val_acc: 0.2907\n",
      "Epoch 7/10\n",
      "37396/37396 [==============================] - 108s - loss: 2.5478 - acc: 0.2990 - val_loss: 2.5862 - val_acc: 0.2939\n",
      "Epoch 8/10\n",
      "37396/37396 [==============================] - 104s - loss: 2.5091 - acc: 0.3074 - val_loss: 2.5607 - val_acc: 0.2983\n",
      "Epoch 9/10\n",
      "37396/37396 [==============================] - 106s - loss: 2.4815 - acc: 0.3116 - val_loss: 2.5509 - val_acc: 0.2998\n",
      "Epoch 10/10\n",
      "37396/37396 [==============================] - 103s - loss: 2.4515 - acc: 0.3192 - val_loss: 2.5403 - val_acc: 0.3040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21628e46b70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cartman'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_prep(words):\n",
    "    tokened = np.array(tokenizer.texts_to_sequences(words))\n",
    "    return pad_sequences(tokened, maxlen=padding)\n",
    "\n",
    "sent = ['Respect my authority']\n",
    "# print(gru_rnn.predict(data_prep(sent)))\n",
    "guess = np.argmax(gru_rnn.predict(data_prep(sent)))\n",
    "new_uniques[guess]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (10 points total)\n",
    "You have free reign to provide additional analyses.\n",
    "### One idea: Use more than a single chain of LSTMs or GRUs (i.e., use multiple parallel chains). \n",
    "Another Idea: Try to create a RNN for generating novel text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
