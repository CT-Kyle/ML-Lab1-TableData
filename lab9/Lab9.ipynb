{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (40 points total)\n",
    "### [20 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 13 ...,  1  3  2]\n",
      "['A Banana' 'A Bishop' 'A Boy' ..., 'Zombie Mortician' 'Zytar' 'al-Zawahri']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "df = pd.read_csv('./south-park-dialogue/All-seasons.csv')\n",
    "\n",
    "y_string = df['Character'].values\n",
    "uniques, y_ints, counts = np.unique(y_string, return_inverse=True,return_counts=True)\n",
    "num_classes = len(uniques)\n",
    "\n",
    "X_preprep = df.drop(['Character', 'Season', 'Episode'], axis=1).values\n",
    "    \n",
    "X_preprep = X_preprep.flatten()\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(y_ints, num_classes)\n",
    "\n",
    "print(counts)\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None\n",
    "padding = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X_preprep)\n",
    "sequences = tokenizer.texts_to_sequences(X_preprep)\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "embed_size = 100\n",
    "\n",
    "f = open('glove.6B/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2, stratify=y_string, random_state=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (50 points total)\n",
    "### [25 points] Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Adjust hyper-parameters of the networks as needed to improve generalization performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embed_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=padding,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2684000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20)                9680      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3950)              82950     \n",
      "=================================================================\n",
      "Total params: 2,776,630\n",
      "Trainable params: 92,630\n",
      "Non-trainable params: 2,684,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Wall time: 742 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "lstm_rnn = Sequential()\n",
    "lstm_rnn.add(embedding_layer)\n",
    "lstm_rnn.add(LSTM(100, dropout=.2, recurrent_dropout=.2))\n",
    "lstm_rnn.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "lstm_rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(lstm_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70896 samples, validate on 70896 samples\n",
      "Epoch 1/3\n",
      "70896/70896 [==============================] - 122s - loss: 5.1229 - acc: 0.1379 - val_loss: 5.0599 - val_acc: 0.1379\n",
      "Epoch 2/3\n",
      "70896/70896 [==============================] - 126s - loss: 5.0985 - acc: 0.1379 - val_loss: 5.0320 - val_acc: 0.1378\n",
      "Epoch 3/3\n",
      "70896/70896 [==============================] - 125s - loss: 5.0787 - acc: 0.1378 - val_loss: 5.0153 - val_acc: 0.1377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e2db91eb8>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_rnn.fit(X, y_ohe, validation_data=(X, y_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70896 samples, validate on 70896 samples\n",
      "Epoch 1/3\n",
      "70896/70896 [==============================] - 122s - loss: 5.0655 - acc: 0.1378 - val_loss: 5.0094 - val_acc: 0.1379\n",
      "Epoch 2/3\n",
      "70896/70896 [==============================] - 120s - loss: 5.0621 - acc: 0.1376 - val_loss: 5.0052 - val_acc: 0.1376\n",
      "Epoch 3/3\n",
      "70896/70896 [==============================] - 126s - loss: 5.0610 - acc: 0.1380 - val_loss: 5.0090 - val_acc: 0.1381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e2fa7a160>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_rnn.fit(X, y_ohe, validation_data=(X, y_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2684000   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 20)                7260      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3950)              82950     \n",
      "=================================================================\n",
      "Total params: 2,774,210\n",
      "Trainable params: 90,210\n",
      "Non-trainable params: 2,684,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "gru_rnn = Sequential()\n",
    "gru_rnn.add(embedding_layer)\n",
    "gru_rnn.add(GRU(100, dropout=.2, recurrent_dropout=.2))\n",
    "gru_rnn.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "gru_rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(gru_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70896 samples, validate on 70896 samples\n",
      "Epoch 1/3\n",
      "70896/70896 [==============================] - 100s - loss: 5.7932 - acc: 0.1374 - val_loss: 5.2541 - val_acc: 0.1379\n",
      "Epoch 2/3\n",
      "70896/70896 [==============================] - 102s - loss: 5.2601 - acc: 0.1379 - val_loss: 5.1481 - val_acc: 0.13791\n",
      "Epoch 3/3\n",
      "70896/70896 [==============================] - 102s - loss: 5.1763 - acc: 0.1377 - val_loss: 5.1042 - val_acc: 0.1366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e31076eb8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_rnn.fit(X, y_ohe, validation_data=(X, y_ohe), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3950)\n"
     ]
    }
   ],
   "source": [
    "def data_prep(words):\n",
    "    tokened = np.array(tokenizer.texts_to_sequences(words))\n",
    "    return pad_sequences(tokened, maxlen=padding)\n",
    "\n",
    "sent = ['hello, my name is Stan']\n",
    "blegh = gru_rnn.predict(data_prep(sent))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (10 points total)\n",
    "You have free reign to provide additional analyses.\n",
    "### One idea: Use more than a single chain of LSTMs or GRUs (i.e., use multiple parallel chains). \n",
    "Another Idea: Try to create a RNN for generating novel text. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
