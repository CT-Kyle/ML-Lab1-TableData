{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (40 points total)\n",
    "### [20 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "df = pd.read_csv('./south-park-dialogue/All-seasons.csv')\n",
    "df = pd.DataFrame(df.drop(['Character', 'Season', 'Episode'], axis=1).values, index=df['Character'])\n",
    "\n",
    "\n",
    "\n",
    "# y_string = df['Character'].values\n",
    "uniques, y_ints, counts = np.unique(y_string, return_inverse=True,return_counts=True)\n",
    "num_classes = len(uniques)\n",
    "\n",
    "# X_preprep = df.drop(['Character', 'Season', 'Episode'], axis=1).values\n",
    "\n",
    "to_delete = []\n",
    "names_to_delete = []\n",
    "\n",
    "character_line_minimum = 100\n",
    "\n",
    "for i, idx in enumerate(counts):\n",
    "    if idx < character_line_minimum:\n",
    "        names_to_delete.append(uniques[i])\n",
    "        to_delete.append(i)\n",
    "\n",
    "df = pd.DataFrame(df.drop(names_to_delete))\n",
    "\n",
    "    \n",
    "y_ints_delete = []\n",
    "for i, idx in enumerate(y_ints):\n",
    "#     print(i, idx)\n",
    "    if idx in uniques[to_delete]:\n",
    "        y_ints_delete.append(i)\n",
    "\n",
    "    \n",
    "print(\"Removing all characters with less than\", character_line_minimum, \"lines\")\n",
    "print(\"Total Characters to Remove:\",len(to_delete))\n",
    "print(\"Total Characters Left: \",len(uniques) - len(to_delete))\n",
    "\n",
    "uniques = np.delete(uniques, to_delete)\n",
    "counts = np.delete(counts, to_delete)\n",
    "\n",
    "print(uniques)\n",
    "print(counts)\n",
    "\n",
    "# y_ints = np.delete(y_ints, y_ints_delete)\n",
    "\n",
    "X_preprep = X_preprep.flatten()\n",
    "\n",
    "num_classes = len(uniques)\n",
    "# y_ohe = keras.utils.to_categorical(y_ints, num_classes)\n",
    "print(y_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_ints_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46745\n",
      "[['You guys, you guys! Chef is going away. \\n']\n",
      " ['Going away? For how long?\\n']\n",
      " ['Forever.\\n']\n",
      " ..., \n",
      " [\"That's not disciprine.\\n\"]\n",
      " ['Right right. Does vodka count?\\n']\n",
      " ['Dad!\\n']]\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "y_string = df.values\n",
    "print(y_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None\n",
    "padding = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X_preprep)\n",
    "sequences = tokenizer.texts_to_sequences(X_preprep)\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=y_string, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (50 points total)\n",
    "### [25 points] Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Adjust hyper-parameters of the networks as needed to improve generalization performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab. Visualize the best results of the RNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (10 points total)\n",
    "You have free reign to provide additional analyses.\n",
    "### One idea: Use more than a single chain of LSTMs or GRUs (i.e., use multiple parallel chains). \n",
    "Another Idea: Try to create a RNN for generating novel text. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
