{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Networks for Dogs and Cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (40 points total)\n",
    "#### [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "image_size = (20, 20)\n",
    "X = []\n",
    "y = []\n",
    "label = [0, 1] #cat == 0, dog == 1\n",
    "for i in range(0, 12000):\n",
    "    image = Image.open('train/cat.' + str(i) +'.jpg')\n",
    "    image = image.resize(image_size)\n",
    "    image = image.convert('L')\n",
    "    image_array = np.array(image).reshape(400,)\n",
    "    X.append(image_array)\n",
    "    y.append(0)\n",
    "    \n",
    "for i in range(0, 12000):\n",
    "    image = Image.open('train/dog.' + str(i) +'.jpg')\n",
    "    image = image.resize(image_size)\n",
    "    image = image.convert('L')\n",
    "    image_array = np.array(image).reshape(400,)\n",
    "    X.append(image_array)\n",
    "    y.append(1)\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 400)\n",
      "(24000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X = X.astype(np.float32) / 255.0 # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [15 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [15 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "y_ohe_train = keras.utils.to_categorical(y_train, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (50 points total)\n",
    "#### [20 points] Create a convolutional neural network to use on your data using tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnn2 = Sequential()\n",
    "cnn2.add(Reshape((1, 20, 20), input_shape=(1, 400)))\n",
    "cnn2.add(Conv2D(filters=16, kernel_size=(2, 2), padding='same'))\n",
    "cnn2.add(Activation('relu'))\n",
    "cnn2.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "cnn2.add(Flatten())\n",
    "cnn2.add(Dense(2))\n",
    "cnn2.add(Activation('softmax'))\n",
    "\n",
    "cnn2.compile(loss='mean_squared_logarithmic_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "cnn2.fit(np.expand_dims(X_train, axis=1), y_ohe_train, batch_size=32, epochs=50, shuffle=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6645833333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "\n",
    "yhat2 = np.argmax(cnn2.predict(np.expand_dims(X_test, axis=1)), axis=1)\n",
    "print(yhat2.shape)\n",
    "acc2 = mt.accuracy_score(y_test, yhat2)\n",
    "acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [20 points] Investigate at least two different deep network architectures. Use the method of cross validation and evaluation metric that you argued for at the beginning of the lab. Visualize the results of the CNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.1212 - acc: 0.5545    \n",
      "Epoch 2/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1175 - acc: 0.5981    \n",
      "Epoch 3/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1149 - acc: 0.6120    \n",
      "Epoch 4/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1138 - acc: 0.6170    \n",
      "Epoch 5/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1120 - acc: 0.6266    \n",
      "Epoch 6/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1113 - acc: 0.6343    \n",
      "Epoch 7/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1103 - acc: 0.6390    \n",
      "Epoch 8/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1100 - acc: 0.6397    \n",
      "Epoch 9/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1090 - acc: 0.6437    \n",
      "Epoch 10/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1085 - acc: 0.6491    \n",
      "Epoch 11/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1073 - acc: 0.6504    \n",
      "Epoch 12/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.1065 - acc: 0.6522    \n",
      "Epoch 13/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1061 - acc: 0.6567    \n",
      "Epoch 14/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1065 - acc: 0.6545    \n",
      "Epoch 15/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1057 - acc: 0.6594    \n",
      "Epoch 16/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1044 - acc: 0.6671    \n",
      "Epoch 17/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1038 - acc: 0.6701    \n",
      "Epoch 18/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1036 - acc: 0.6678    \n",
      "Epoch 19/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1036 - acc: 0.6703    \n",
      "Epoch 20/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1029 - acc: 0.6722    \n",
      "Epoch 21/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1023 - acc: 0.6746    \n",
      "Epoch 22/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1023 - acc: 0.6751    \n",
      "Epoch 23/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1022 - acc: 0.6730    \n",
      "Epoch 24/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1016 - acc: 0.6759    \n",
      "Epoch 25/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1011 - acc: 0.6817    \n",
      "Epoch 26/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1011 - acc: 0.6793    \n",
      "Epoch 27/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1004 - acc: 0.6834    \n",
      "Epoch 28/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1004 - acc: 0.6796    \n",
      "Epoch 29/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.1007 - acc: 0.6833    \n",
      "Epoch 30/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.1002 - acc: 0.6861    \n",
      "Epoch 31/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0995 - acc: 0.6880    \n",
      "Epoch 32/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0997 - acc: 0.6856    \n",
      "Epoch 33/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0998 - acc: 0.6877    \n",
      "Epoch 34/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0986 - acc: 0.6902    \n",
      "Epoch 35/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0984 - acc: 0.6907    \n",
      "Epoch 36/75\n",
      "19200/19200 [==============================] - ETA: 0s - loss: 0.0984 - acc: 0.693 - 10s - loss: 0.0984 - acc: 0.6935    \n",
      "Epoch 37/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.0983 - acc: 0.6908    \n",
      "Epoch 38/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0984 - acc: 0.6935    \n",
      "Epoch 39/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0981 - acc: 0.6929    \n",
      "Epoch 40/75\n",
      "19200/19200 [==============================] - 13s - loss: 0.0969 - acc: 0.6972    \n",
      "Epoch 41/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0980 - acc: 0.6916    \n",
      "Epoch 42/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0980 - acc: 0.6955    \n",
      "Epoch 43/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0973 - acc: 0.6970    \n",
      "Epoch 44/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0964 - acc: 0.7004    \n",
      "Epoch 45/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0977 - acc: 0.6957    \n",
      "Epoch 46/75\n",
      "19200/19200 [==============================] - 10s - loss: 0.0967 - acc: 0.6971    \n",
      "Epoch 47/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0965 - acc: 0.7003    \n",
      "Epoch 48/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0958 - acc: 0.7035    \n",
      "Epoch 49/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0970 - acc: 0.6955    \n",
      "Epoch 50/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0962 - acc: 0.7004    \n",
      "Epoch 51/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0953 - acc: 0.7058    \n",
      "Epoch 52/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0957 - acc: 0.7034    \n",
      "Epoch 53/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0958 - acc: 0.6999    \n",
      "Epoch 54/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0961 - acc: 0.7021    \n",
      "Epoch 55/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0952 - acc: 0.7049    \n",
      "Epoch 56/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0950 - acc: 0.7075    \n",
      "Epoch 57/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0952 - acc: 0.7067    \n",
      "Epoch 58/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0948 - acc: 0.7065    \n",
      "Epoch 59/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0946 - acc: 0.7085    \n",
      "Epoch 60/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0944 - acc: 0.7086    \n",
      "Epoch 61/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0943 - acc: 0.7091    \n",
      "Epoch 62/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0938 - acc: 0.7081    \n",
      "Epoch 63/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0944 - acc: 0.7092    \n",
      "Epoch 64/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0932 - acc: 0.7148    \n",
      "Epoch 65/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0941 - acc: 0.7087    \n",
      "Epoch 66/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0930 - acc: 0.7143    \n",
      "Epoch 67/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0928 - acc: 0.7152    \n",
      "Epoch 68/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0943 - acc: 0.7084    \n",
      "Epoch 69/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0929 - acc: 0.7141    \n",
      "Epoch 70/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0931 - acc: 0.7140    \n",
      "Epoch 71/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0928 - acc: 0.7130    \n",
      "Epoch 72/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0931 - acc: 0.7148    \n",
      "Epoch 73/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0921 - acc: 0.7183    \n",
      "Epoch 74/75\n",
      "19200/19200 [==============================] - 11s - loss: 0.0923 - acc: 0.7160    \n",
      "Epoch 75/75\n",
      "19200/19200 [==============================] - 12s - loss: 0.0931 - acc: 0.7136    \n",
      "Wall time: 14min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Reshape((1, 20, 20), input_shape=(1, 400)))\n",
    "cnn.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "\n",
    "cnn.add(Dropout(.25))\n",
    "cnn.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(Dropout(.25))\n",
    "cnn.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "\n",
    "cnn.add(Dropout(.2))\n",
    "cnn.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(Dropout(.2))\n",
    "cnn.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "cnn.add(Activation('relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n",
    "\n",
    "\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(2))\n",
    "cnn.add(Activation('softmax'))\n",
    "\n",
    "cnn.compile(loss='mean_squared_logarithmic_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "cnn.fit(np.expand_dims(X_train, axis=1), y_ohe_train, batch_size=32, epochs=50, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66645833333333337"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = np.argmax(cnn.predict(np.expand_dims(X_test, axis=1)), axis=1)\n",
    "acc = mt.accuracy_score(y_test, yhat)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [10 points] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27387be7a20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add( Dense(input_dim=X_train.shape[1], units=30, activation='relu') )\n",
    "mlp.add( Dense(units=15, activation='relu') )\n",
    "mlp.add( Dense(2) )\n",
    "mlp.add( Activation('softmax') )\n",
    "\n",
    "mlp.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp.fit(X_train, y_ohe_train, \n",
    "        batch_size=32, epochs=150, \n",
    "        shuffle=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60291666666666666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_mlp = np.argmax(mlp.predict(X_test), axis=1)\n",
    "acc_mlp = mt.accuracy_score(y_test,yhat_mlp)\n",
    "acc_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (10 points total)\n",
    "#### One idea: Visualize the convolutional filters chosen by your CNN. Try to interpret some of their meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensor]",
   "language": "python",
   "name": "conda-env-tensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
